WEBVTT

00:00:22.059 --> 00:00:25.646 align:center
What's the most important century
in human history?

00:00:26.105 --> 00:00:30.025 align:center
Some might argue it’s a period
of extensive military campaigning,

00:00:30.025 --> 00:00:33.570 align:center
like Alexander the Great’s
in the 300s BCE,

00:00:33.904 --> 00:00:37.157 align:center
which reshaped political
and cultural borders.

00:00:38.992 --> 00:00:42.079 align:center
Others might cite the emergence
of a major religion,

00:00:42.079 --> 00:00:44.206 align:center
such as Islam in the 7th century,

00:00:44.581 --> 00:00:48.085 align:center
which codified and spread values
across such borders.

00:00:50.587 --> 00:00:54.049 align:center
Or perhaps it’s the Industrial Revolution
of the 1700s

00:00:54.049 --> 00:00:55.801 align:center
that transformed global commerce

00:00:55.801 --> 00:00:58.929 align:center
and redefined humanity's relationship
with labor.

00:00:59.221 --> 00:01:03.350 align:center
Whatever the answer, it seems like
any century vying for that top spot

00:01:03.350 --> 00:01:05.727 align:center
is at a moment of great change—

00:01:06.687 --> 00:01:10.858 align:center
when the actions of our ancestors shifted
humanity’s trajectory

00:01:10.858 --> 00:01:12.442 align:center
for centuries to come.

00:01:12.776 --> 00:01:16.697 align:center
So if this is our metric,
is it possible that right now—

00:01:16.697 --> 00:01:19.783 align:center
this century—
is the most important one yet?

00:01:20.826 --> 00:01:26.039 align:center
The 21st century has already proven to be
a period of rapid technological growth.

00:01:26.290 --> 00:01:29.418 align:center
Phones and computers have accelerated
the pace of life.

00:01:29.459 --> 00:01:33.630 align:center
And we’re likely on the cusp of developing
new transformative technologies,

00:01:33.630 --> 00:01:35.924 align:center
like advanced artificial intelligence,

00:01:35.924 --> 00:01:38.802 align:center
that could entirely change
the way people live.

00:01:40.470 --> 00:01:43.515 align:center
Meanwhile, many technologies
we already have

00:01:43.515 --> 00:01:48.353 align:center
contribute to humanity’s unprecedented
levels of existential risk—

00:01:48.437 --> 00:01:50.814 align:center
that’s the risk of our
species going extinct

00:01:50.814 --> 00:01:54.234 align:center
or experiencing some kind of disaster
that permanently limits

00:01:54.234 --> 00:01:56.904 align:center
humanity’s ability to grow and thrive.

00:01:58.322 --> 00:02:02.951 align:center
The invention of the atomic bomb marked
a major rise in existential risk,

00:02:03.452 --> 00:02:07.331 align:center
and since then we’ve only increased
the odds against us.

00:02:07.956 --> 00:02:10.584 align:center
It’s profoundly difficult
to estimate the odds

00:02:10.584 --> 00:02:13.212 align:center
of an existential collapse
occurring this century.

00:02:13.295 --> 00:02:16.757 align:center
Very rough guesses put the risk
of existential catastrophe

00:02:16.757 --> 00:02:21.637 align:center
due to nuclear winter and climate change
at around 0.1%,

00:02:23.347 --> 00:02:26.558 align:center
with the odds of a pandemic causing
the same kind of collapse

00:02:26.558 --> 00:02:28.518 align:center
at a frightening 3%.

00:02:29.061 --> 00:02:33.941 align:center
Given that any of these disasters could
mean the end of life as we know it,

00:02:34.233 --> 00:02:36.360 align:center
these aren’t exactly small figures,

00:02:36.360 --> 00:02:40.364 align:center
And it’s possible this century could see
the rise of new technologies

00:02:40.364 --> 00:02:43.158 align:center
that introduce more existential risks.

00:02:44.159 --> 00:02:46.787 align:center
AI experts have a wide range
of estimates regarding

00:02:46.787 --> 00:02:49.748 align:center
when artificial general intelligence
will emerge,

00:02:49.790 --> 00:02:53.877 align:center
but according to some surveys,
many believe it could happen this century.

00:02:54.711 --> 00:02:58.548 align:center
Currently, we have relatively narrow forms
of artificial intelligence,

00:02:58.548 --> 00:03:03.136 align:center
which are designed to do specific tasks
like play chess or recognize faces.

00:03:04.096 --> 00:03:09.309 align:center
Even narrow AIs that do creative work are
limited to their singular specialty.

00:03:09.559 --> 00:03:13.188 align:center
But artificial general intelligences,
or AGIs,

00:03:13.188 --> 00:03:17.025 align:center
would be able to adapt to and
perform any number of tasks,

00:03:17.734 --> 00:03:20.612 align:center
quickly outpacing
their human counterparts.

00:03:21.905 --> 00:03:25.993 align:center
There are a huge variety of guesses
about what AGI could look like,

00:03:26.702 --> 00:03:29.538 align:center
and what it would mean
for humanity to share the Earth

00:03:29.538 --> 00:03:31.665 align:center
with another sentient entity.

00:03:33.709 --> 00:03:36.253 align:center
AGIs might help us achieve our goals,

00:03:36.253 --> 00:03:38.547 align:center
they might regard us as inconsequential,

00:03:38.547 --> 00:03:41.717 align:center
or, they might see us as an obstacle
to swiftly remove.

00:03:41.925 --> 00:03:44.094 align:center
So in terms of existential risk,

00:03:44.094 --> 00:03:48.181 align:center
it's imperative the values of this new
technology align with our own.

00:03:48.557 --> 00:03:52.436 align:center
This is an incredibly difficult
philosophical and engineering challenge

00:03:52.436 --> 00:03:55.522 align:center
that will require a lot
of delicate, thoughtful work.

00:03:56.023 --> 00:04:01.236 align:center
Yet, even if we succeed, AGI could still
lead to another complicated outcome.

00:04:01.903 --> 00:04:05.699 align:center
Let’s imagine an AGI emerges
with deep respect for human life

00:04:05.699 --> 00:04:08.660 align:center
and a desire to solve
all humanity’s troubles.

00:04:10.287 --> 00:04:12.456 align:center
But to avoid becoming misaligned,

00:04:12.456 --> 00:04:16.084 align:center
it's been developed to be incredibly
rigid about its beliefs.

00:04:16.293 --> 00:04:19.379 align:center
If these machines became
the dominant power on Earth,

00:04:19.379 --> 00:04:22.090 align:center
their strict values might
become hegemonic,

00:04:22.090 --> 00:04:27.304 align:center
locking humanity into one ideology that
would be incredibly resistant to change.

00:04:29.848 --> 00:04:32.351 align:center
History has taught us that
no matter how enlightened

00:04:32.351 --> 00:04:34.144 align:center
a civilization thinks they are,

00:04:34.144 --> 00:04:37.939 align:center
they are rarely up to the moral standards
of later generations.

00:04:38.065 --> 00:04:42.694 align:center
And this kind of value lock in
could permanently distort or constrain

00:04:42.694 --> 00:04:44.613 align:center
humanity’s moral growth.

00:04:45.072 --> 00:04:47.657 align:center
There's a ton of uncertainty around AGI,

00:04:47.657 --> 00:04:51.370 align:center
and it’s profoundly difficult to predict
how any existential risks

00:04:51.370 --> 00:04:53.372 align:center
will play out over the next century.

00:04:53.705 --> 00:04:56.625 align:center
It’s also possible that new,
more pressing concerns

00:04:56.625 --> 00:04:58.627 align:center
might render these risks moot.

00:04:58.960 --> 00:05:03.673 align:center
But even if we can't definitively say that
ours is the most important century,

00:05:03.673 --> 00:05:07.427 align:center
it still seems like the decisions
we make might have a major impact

00:05:07.427 --> 00:05:08.929 align:center
on humanity’s future.

00:05:09.471 --> 00:05:12.682 align:center
So maybe we should all live
like the future depends on us—

00:05:12.682 --> 00:05:14.893 align:center
because actually, it just might.

