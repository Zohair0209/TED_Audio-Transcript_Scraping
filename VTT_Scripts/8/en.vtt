WEBVTT

00:00:21.391 --> 00:00:23.560 align:center
In the coming years,
artificial intelligence

00:00:23.560 --> 00:00:27.564 align:center
is probably going to change your life,
and likely the entire world.

00:00:27.648 --> 00:00:30.776 align:center
But people have a hard time
agreeing on exactly how.

00:00:30.776 --> 00:00:33.987 align:center
The following are excerpts
from a World Economic Forum interview

00:00:33.987 --> 00:00:37.783 align:center
where renowned computer science professor
and AI expert Stuart Russell

00:00:37.783 --> 00:00:40.369 align:center
helps separate the sense
from the nonsense.

00:00:40.369 --> 00:00:44.123 align:center
There’s a big difference between asking
a human to do something

00:00:44.123 --> 00:00:47.584 align:center
and giving that as the objective
to an AI system.

00:00:47.584 --> 00:00:50.212 align:center
When you ask a human to get
you a cup of coffee,

00:00:50.212 --> 00:00:52.798 align:center
you don’t mean this should be
their life’s mission,

00:00:52.798 --> 00:00:54.758 align:center
and nothing else in the universe matters.

00:00:54.758 --> 00:00:57.344 align:center
Even if they have to kill everybody else
in Starbucks

00:00:57.344 --> 00:01:00.180 align:center
to get you the coffee before it closes—
they should do that.

00:01:00.180 --> 00:01:01.807 align:center
No, that’s not what you mean.

00:01:01.807 --> 00:01:04.101 align:center
All the other things that
we mutually care about,

00:01:04.101 --> 00:01:06.270 align:center
they should factor
into your behavior as well.

00:01:06.270 --> 00:01:09.439 align:center
And the problem with the way
we build AI systems now

00:01:09.439 --> 00:01:11.066 align:center
is we give them a fixed objective.

00:01:11.066 --> 00:01:14.611 align:center
The algorithms require us
to specify everything in the objective.

00:01:14.611 --> 00:01:18.031 align:center
And if you say, can we fix the
acidification of the oceans?

00:01:18.031 --> 00:01:22.077 align:center
Yeah, you could have a catalytic reaction
that does that extremely efficiently,

00:01:22.077 --> 00:01:25.330 align:center
but it consumes a quarter
of the oxygen in the atmosphere,

00:01:25.330 --> 00:01:29.042 align:center
which would apparently cause us to die
fairly slowly and unpleasantly

00:01:29.042 --> 00:01:30.794 align:center
over the course of several hours.

00:01:30.836 --> 00:01:34.047 align:center
So, how do we avoid this problem?

00:01:34.047 --> 00:01:38.135 align:center
You might say, okay, well, just be more
careful about specifying the objective—

00:01:38.135 --> 00:01:40.679 align:center
don’t forget the atmospheric oxygen.

00:01:40.929 --> 00:01:44.474 align:center
And then, of course, some side effect
of the reaction in the ocean

00:01:44.474 --> 00:01:45.851 align:center
poisons all the fish.

00:01:45.851 --> 00:01:48.520 align:center
Okay, well I meant don’t kill
the fish either.

00:01:48.520 --> 00:01:50.439 align:center
And then, well, what about
the seaweed?

00:01:50.439 --> 00:01:53.400 align:center
Don’t do anything that’s going
to cause all the seaweed to die.

00:01:53.400 --> 00:01:54.610 align:center
And on and on and on.

00:01:54.735 --> 00:01:58.655 align:center
And the reason that we don’t have to do
that with humans is that

00:01:58.655 --> 00:02:03.160 align:center
humans often know that they don’t know
all the things that we care about.

00:02:03.410 --> 00:02:06.371 align:center
If you ask a human to get you
a cup of coffee,

00:02:06.371 --> 00:02:09.249 align:center
and you happen to be
in the Hotel George Sand in Paris,

00:02:09.249 --> 00:02:11.877 align:center
where the coffee is 13 euros a cup,

00:02:11.877 --> 00:02:16.048 align:center
it’s entirely reasonable to come
back and say, well, it’s 13 euros,

00:02:16.048 --> 00:02:19.009 align:center
are you sure you want it,
or I could go next door and get one?

00:02:19.009 --> 00:02:21.887 align:center
And it’s a perfectly normal thing
for a person to do.

00:02:22.095 --> 00:02:25.098 align:center
To ask, I’m going to repaint your house—

00:02:25.098 --> 00:02:28.435 align:center
is it okay if I take off the drainpipes
and then put them back?

00:02:28.560 --> 00:02:31.688 align:center
We don't think of this as a terribly
sophisticated capability,

00:02:31.688 --> 00:02:34.775 align:center
but AI systems don’t have it
because the way we build them now,

00:02:34.775 --> 00:02:36.568 align:center
they have to know the full objective.

00:02:36.777 --> 00:02:40.530 align:center
If we build systems that know that
they don’t know what the objective is,

00:02:40.530 --> 00:02:43.116 align:center
then they start to exhibit
these behaviors,

00:02:43.116 --> 00:02:47.162 align:center
like asking permission before getting rid
of all the oxygen in the atmosphere.

00:02:47.621 --> 00:02:50.999 align:center
In all these senses,
control over the AI system

00:02:50.999 --> 00:02:55.462 align:center
comes from the machine’s uncertainty
about what the true objective is.

00:02:56.338 --> 00:02:59.424 align:center
And it’s when you build machines that
believe with certainty

00:02:59.424 --> 00:03:00.842 align:center
that they have the objective,

00:03:00.842 --> 00:03:03.595 align:center
that’s when you get this
sort of psychopathic behavior.

00:03:03.595 --> 00:03:05.722 align:center
And I think we see
the same thing in humans.

00:03:05.806 --> 00:03:10.060 align:center
What happens when general purpose AI
hits the real economy?

00:03:10.435 --> 00:03:14.022 align:center
How do things change? Can we adapt?

00:03:14.231 --> 00:03:16.066 align:center
This is a very old point.

00:03:16.066 --> 00:03:19.653 align:center
Amazingly, Aristotle actually has
a passage where he says,

00:03:19.653 --> 00:03:22.698 align:center
look, if we had fully automated
weaving machines

00:03:22.698 --> 00:03:26.535 align:center
and plectrums that could pluck the lyre
and produce music without any humans,

00:03:26.660 --> 00:03:28.662 align:center
then we wouldn’t need any workers.

00:03:28.870 --> 00:03:31.748 align:center
That idea, which I think it was Keynes

00:03:31.748 --> 00:03:34.584 align:center
who called it technological unemployment
in 1930,

00:03:34.584 --> 00:03:36.503 align:center
is very obvious to people.

00:03:36.503 --> 00:03:39.589 align:center
They think, yeah, of course,
if the machine does the work,

00:03:39.589 --> 00:03:41.258 align:center
then I'm going to be unemployed.

00:03:41.425 --> 00:03:44.928 align:center
You can think about the warehouses
that companies are currently operating

00:03:44.928 --> 00:03:47.639 align:center
for e-commerce,
they are half automated.

00:03:47.639 --> 00:03:51.685 align:center
The way it works is that an old warehouse—
where you’ve got tons of stuff piled up

00:03:51.685 --> 00:03:54.146 align:center
all over the place
and humans go and rummage around

00:03:54.146 --> 00:03:56.023 align:center
and then bring it back and send it off—

00:03:56.023 --> 00:03:59.609 align:center
there’s a robot who goes
and gets the shelving unit

00:03:59.609 --> 00:04:01.528 align:center
that contains the thing that you need,

00:04:01.528 --> 00:04:05.157 align:center
but the human has to pick the object
out of the bin or off the shelf,

00:04:05.157 --> 00:04:07.034 align:center
because that’s still too difficult.

00:04:07.075 --> 00:04:09.077 align:center
But, at the same time,

00:04:09.077 --> 00:04:12.998 align:center
would you make a robot that is accurate
enough to be able to pick

00:04:12.998 --> 00:04:17.336 align:center
pretty much any object within a very wide
variety of objects that you can buy?

00:04:17.336 --> 00:04:21.340 align:center
That would, at a stroke,
eliminate 3 or 4 million jobs?

00:04:21.465 --> 00:04:24.801 align:center
There's an interesting story
that E.M. Forster wrote,

00:04:24.801 --> 00:04:28.305 align:center
where everyone is entirely
machine dependent.

00:04:28.555 --> 00:04:32.309 align:center
The story is really about the
fact that if you hand over

00:04:32.309 --> 00:04:35.270 align:center
the management of your civilization
to machines,

00:04:35.270 --> 00:04:38.774 align:center
you then lose the incentive to understand
it yourself

00:04:38.774 --> 00:04:41.318 align:center
or to teach the next generation
how to understand it.

00:04:41.318 --> 00:04:44.321 align:center
You can see “WALL-E”
actually as a modern version,

00:04:44.321 --> 00:04:47.949 align:center
where everyone is enfeebled
and infantilized by the machine,

00:04:47.949 --> 00:04:49.910 align:center
and that hasn’t been possible
up to now.

00:04:49.910 --> 00:04:52.329 align:center
We put a lot of our civilization
into books,

00:04:52.329 --> 00:04:53.955 align:center
but the books can’t run it for us.

00:04:53.955 --> 00:04:56.750 align:center
And so we always have to teach
the next generation.

00:04:56.792 --> 00:05:01.004 align:center
If you work it out, it’s about a trillion
person years of teaching and learning

00:05:01.004 --> 00:05:04.966 align:center
and an unbroken chain that goes back
tens of thousands of generations.

00:05:05.175 --> 00:05:07.094 align:center
What happens if that chain breaks?

00:05:07.094 --> 00:05:10.555 align:center
I think that’s something we have
to understand as AI moves forward.

00:05:10.680 --> 00:05:14.267 align:center
The actual date of arrival
of general purpose AI—

00:05:14.267 --> 00:05:17.354 align:center
you’re not going to be able to pinpoint,
it isn’t a single day.

00:05:17.354 --> 00:05:19.648 align:center
It’s also not the case
that it’s all or nothing.

00:05:19.648 --> 00:05:22.109 align:center
The impact is going to be increasing.

00:05:22.109 --> 00:05:24.152 align:center
So with every advance in AI,

00:05:24.152 --> 00:05:27.114 align:center
it significantly expands
the range of tasks.

00:05:27.114 --> 00:05:32.452 align:center
So in that sense, I think most experts say
by the end of the century,

00:05:32.452 --> 00:05:35.789 align:center
we’re very, very likely to have
general purpose AI.

00:05:35.789 --> 00:05:39.543 align:center
The median is something around 2045.

00:05:39.543 --> 00:05:41.545 align:center
I'm a little more on the
conservative side.

00:05:41.545 --> 00:05:43.630 align:center
I think the problem is
harder than we think.

00:05:43.630 --> 00:05:46.883 align:center
I like what John McAfee,
he was one of the founders of AI,

00:05:46.967 --> 00:05:50.804 align:center
when he was asked this question, he said,
somewhere between five and 500 years.

00:05:50.804 --> 00:05:54.141 align:center
And we're going to need, I think, several
Einsteins to make it happen.

